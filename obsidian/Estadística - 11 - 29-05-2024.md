Inferencia bayesiana:

La diferencia es, a diferencia de estimar un parametro, se estima una distribución de probabilidad.

Andrew Gelman: "Los metodos bayesianos nos permiten realizar afirmaciones acerca del conocmiento parcial dispinible (datos) que temenmos acerca de alguna situación o "state of nature" (observable o aun no observada) de manera sistematica, usando probabilidades"

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$

Es altamente costoso en términos de algebra

Ejemplo Binomial:
$$ P(y|\theta) \propto \theta^y (1-\theta)^{n-y} $$
$$ P(\theta) \sim U(0,1) $$
$$ P(\theta|y) \propto \theta^y (1-\theta)^{n-y} $$
$$ P(\theta|y) \sim Beta(y+1, n-y+1) $$

Consideraciones:
$$ \theta \sim Beta(\alpha, \beta) $$
$$ E(\theta) = \frac{\alpha}{\alpha + \beta} $$
$$ Var(\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} $$
Cuando el prior es conjugado es cuando distribuye igual que la posterior

Definición de prior conjugado:


Otra diferencia con el mundo frecuentista: Intervalos de confianza: en bayesiano se llama: región de alta densidad.

Priors conjugados:

Familia exponencial: (es una familia de distribuciones)Definimos la siguiente funcion de densidad de probabilidad parametrizada por:

La familia de distribuciones que son parte de esta familia exponencial: Normal, exponenial, gamma, chi-cuadrado, beta, dirichlet, bernoulli, poisson y geométrica

Concepto de suficiencia: un estimador debiese ser una herramienta estadistica que es capaz de resumir bien la variable aleatoria. Captura la mayor parte de información posible para estimar la variable aleatoria.

Definición: Prior propio

Decimos que un prior P(THETA) es propio si no depende de los datos e intengra 1. Si p(theta) integra a cualquier valor finito positivo entonces se le llama una densidad no normalizada que se puede normalizar para que la integral sea 1


![[./Recording 20240529160006.webm|Recording 20240529160006]]



