"Como prior voy a asumor una uniforme e n mu y una uniforme en log de sigma"

Luego de todo el procedimiento matemático se llega al resultado de sigma cuadrado elevado a menos -1"

Recordar que este resultado es para mu desconocido y sigma cuadrado desconocido, es decir, cuando desconocemos los parametros de dispersión de la muestra.

Recordar:

- Para el caso 1: $$
P(\sigma^2|y) \approx \text{inv-}\chi^2(n-1, S^2)
$$
- Para el caso 2: $$
P(\mu|y) \propto \text{Normal}(\hat{y}, \sigma^2/n)
$$
- En caso 3 es la varianza es lo mismo cuando es conocida.

Para evaluar se tiene que tomar muestras

"Procesamiento de lenguaje natural" es un electivo del MDS.

Modelos de semántica latente, la producción de documentos se tratas de temas latentes. Esto se dio con un modelo bayesiano de 3 capas.

***Existen los Modelos de Semántica Latente***

Blei, 2003.

Sabemos que una palabra es mas probable dado un tópico:$$
P(W_n|Z_n)
$$
Sabemos que un tópico es mas probable dado un documento:
$$
P(Z_n|\theta)
$$
Como hiperparámetros:$$\Theta \sim \text{Dirichlet}(\alpha)
$$$$Z_n \sim \text{Categorical}(\Theta)
$$
$$\Phi \sim \text{Dirichlet}(\beta)$$
$$W_n \sim \text{Categorical}(\Phi_{Z_n})$$
VERtopic: encuentra clases latentes. en python. es el futuro del procesamiento del lenguaje natural.

Intercambiabiblidad: es una propiedad, un sampleo independiente y valores para cada uno.
Exchangeability: Los parámetros son intercambiables en su distribución conjunta, si p es invariante a la premutación de indices:
- Lla forma mas simple es asumir que existe
- Un parametro Nabla que no conocemos tal que:


Posteriors o distribuciones como mixturas de parámetros independientes.

Finetti's theorem: Cuando J -> infinto cualquier distribución bien portada intercambiable puede ser expresada como una mixtura de distribuciones identicas e independientes.


Bien portada: que se comportan bien matemáticamente.

Intercambiabilidad no es lo mismo que ser iid. Es como decir: me da lo mismo el orden en que hago la estimación. Simplifica la inferencia

Cadena de Markov: se utiliza en procesos estocasticos de los estados en el cual se va transformando la variable aleatoria.

Definición profesora: Es un proceso estocástico que describe una serie de posibles elementos donde la probabilidad de cada evento solo depende del evento o eventos anterior. En otras palabras condicionando el presente, el futuro y el pasado son independientes. Formalmente:
$$
P(X_{n+1} = e_{n+1}|X_k = e_k, k=1,..., n)
$$

$$
P(X_{n+1} = e_{n+1}|X_n = e_n)
$$


![[./Recording 20240605160323.webm|Recording 20240605160323]]


Recordar que no todos los valores son iid.







