
Función no lineal clasica: Relu.

Cuando aplico funciones no lineales, rompo la cadena.

Es por eso que se pueden aproximar, porque.

Tiene que ser consensual a los datos que quermos predecir.

La y sombrero depende de los parametros de la red neuronal.

Minimizar la pérdida.

Backpropagation encuentra gradientes.

Se encuentran los gradientes a traves de la regla de la cadena, que no es autoregresiva. transforma las posiciones de transformaciones 

Sigmoide regresion logistica es de 0 a 1.

Heavysitde.

En el caso de relu estamos componiendo puras funciones as.

"Consiste en tirar tod

Teorira es un con junto de teorias, aplica a utilizar a una funcion

Hint.

Diagonalizar matrices.

Recordar que hay vectores invariantes, es decir que no se altera su direccion, tambien son llamados vectores propios.

Lambda = vector propio.

Tenemos que saber que el vector = 0 es solucion.

Necesitamos saber que el determintante de A + lambda de I = 0

El determinante de algo es un polinomio.

El determinante de una dos por dos es multiplicar cruzado.

Determinante de tres por tres hay que factorizar.

Valores propios = eigenvalues = valores espectros

El resultado es "todas las generaciones ponderadas por este conjunto"

Es imporante diagonalizar el PCA

Ridge para cuando cree que los datos tienen poca varianza.


![[./Recording 20240611132759.webm|Recording 20240611132759]]


